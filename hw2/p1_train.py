# -*- coding: utf-8 -*-
from tqdm import tqdm
import logging
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable
from torch import optim
import torchvision.transforms as transforms
import torchvision
import torch.nn.functional as F
import torch.nn as nn
import torch
from qqdm.notebook import qqdm
from datetime import datetime
import random
import glob
import os
from PIL import Image

# from torch.nn.modules import conv
# from torch.nn.modules.utils import _pair
# from torch.nn.modules import Linear

"""DLCVHW2_p1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fWhnKrO7x27nX_Z2kiz1JzOVvHyFNkXZ

## Download Dataset
"""

# !nvidia-smi

# # get dataset from huggingface hub
# !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash
# !apt-get install git-lfs
# !git lfs install
# !git clone https://huggingface.co/datasets/LeoFeng/MLHW_6
# !unzip ./MLHW_6/faces.zip -d .

# !gdown --id 1YxkObGDlqZM0-9Zq-QMjk7q1vND4UJl3 --output "data.zip"
# ! unzip data.zip

"""## Other setting"""

# import module


# seed setting

def same_seeds(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


same_seeds(1126)

workspace_dir = './hw2_data/face/'

"""# Dataset preparation
In this section, we prepare for the dataset for Pytorch

## Create dataset for Pytorch

In order to unified image information, we use the transform function to:
1. Resize image to 64x64
2. Normalize the image

This CrypkoDataset class will be use in Section 4
"""


class p1Data(Dataset):
    def __init__(self, fnames, transform):
        self.transform = transform
        self.fnames = fnames
        self.num_samples = len(self.fnames)

    def __getitem__(self, idx):
        fname = self.fnames[idx]
        img = torchvision.io.read_image(fname)
        img = self.transform(img)
        return img

    def __len__(self):
        return self.num_samples


def get_dataset(root):
    fnames = glob.glob(os.path.join(root, '*'))
    compose = [
        transforms.ToPILImage(),
        transforms.Resize((64, 64)),
        transforms.ToTensor(),
        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
    ]
    transform = transforms.Compose(compose)
    dataset = p1Data(fnames, transform)
    return dataset


"""## Show the image
Show some sample in the dataset
"""

dataset = get_dataset(os.path.join(workspace_dir, 'train'))

images = [dataset[i] for i in range(16)]
grid_img = torchvision.utils.make_grid(images, nrow=4)
plt.figure(figsize=(10, 10))
plt.imshow(grid_img.permute(1, 2, 0))
plt.show()

"""## Model
Here, we use DCGAN as the model structure. Feel free to modify your own model structure.

Note that the `N` of the input/output shape stands for the batch size.
"""


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)


# def _l2normalize(v, eps=1e-12):
#     return v / (torch.norm(v) + eps)


# def max_singular_value(W, u=None, Ip=1):
#     """
#     power iteration for weight parameter
#     """
#     #xp = W.data
#     if not Ip >= 1:
#         raise ValueError("Power iteration should be a positive integer")
#     if u is None:
#         u = torch.FloatTensor(1, W.size(0)).normal_(0, 1).to(device)
#     _u = u
#     for _ in range(Ip):
#         _v = _l2normalize(torch.matmul(_u, W.data), eps=1e-12)
#         _u = _l2normalize(torch.matmul(
#             _v, torch.transpose(W.data, 0, 1)), eps=1e-12)
#     sigma = torch.sum(F.linear(_u, torch.transpose(W.data, 0, 1)) * _v)
#     return sigma, _u


# class SNConv2d(conv._ConvNd):

#     r"""Applies a 2D convolution over an input signal composed of several input
#     planes.
#     In the simplest case, the output value of the layer with input size
#     :math:`(N, C_{in}, H, W)` and output :math:`(N, C_{out}, H_{out}, W_{out})`
#     can be precisely described as:
#     .. math::
#         \begin{array}{ll}
#         out(N_i, C_{out_j})  = bias(C_{out_j})
#                        + \sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \star input(N_i, k)
#         \end{array}
#     where :math:`\star` is the valid 2D `cross-correlation`_ operator,
#     :math:`N` is a batch size, :math:`C` denotes a number of channels,
#     :math:`H` is a height of input planes in pixels, and :math:`W` is
#     width in pixels.
#     | :attr:`stride` controls the stride for the cross-correlation, a single
#       number or a tuple.
#     | :attr:`padding` controls the amount of implicit zero-paddings on both
#     |  sides for :attr:`padding` number of points for each dimension.
#     | :attr:`dilation` controls the spacing between the kernel points; also
#       known as the à trous algorithm. It is harder to describe, but this `link`_
#       has a nice visualization of what :attr:`dilation` does.
#     | :attr:`groups` controls the connections between inputs and outputs.
#       `in_channels` and `out_channels` must both be divisible by `groups`.
#     |       At groups=1, all inputs are convolved to all outputs.
#     |       At groups=2, the operation becomes equivalent to having two conv
#                  layers side by side, each seeing half the input channels,
#                  and producing half the output channels, and both subsequently
#                  concatenated.
#             At groups=`in_channels`, each input channel is convolved with its
#                  own set of filters (of size `out_channels // in_channels`).
#     The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:
#         - a single ``int`` -- in which case the same value is used for the height and width dimension
#         - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
#           and the second `int` for the width dimension
#     .. note::
#          Depending of the size of your kernel, several (of the last)
#          columns of the input might be lost, because it is a valid `cross-correlation`_,
#          and not a full `cross-correlation`_.
#          It is up to the user to add proper padding.
#     .. note::
#          The configuration when `groups == in_channels` and `out_channels = K * in_channels`
#          where `K` is a positive integer is termed in literature as depthwise convolution.
#          In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`, if you want a
#          depthwise convolution with a depthwise multiplier `K`,
#          then you use the constructor arguments
#          :math:`(in\_channels=C_{in}, out\_channels=C_{in} * K, ..., groups=C_{in})`
#     Args:
#         in_channels (int): Number of channels in the input image
#         out_channels (int): Number of channels produced by the convolution
#         kernel_size (int or tuple): Size of the convolving kernel
#         stride (int or tuple, optional): Stride of the convolution. Default: 1
#         padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0
#         dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
#         groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
#         bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
#     Shape:
#         - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
#         - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where
#           :math:`H_{out} = floor((H_{in}  + 2 * padding[0] - dilation[0] * (kernel\_size[0] - 1) - 1) / stride[0] + 1)`
#           :math:`W_{out} = floor((W_{in}  + 2 * padding[1] - dilation[1] * (kernel\_size[1] - 1) - 1) / stride[1] + 1)`
#     Attributes:
#         weight (Tensor): the learnable weights of the module of shape
#                          (out_channels, in_channels, kernel_size[0], kernel_size[1])
#         bias (Tensor):   the learnable bias of the module of shape (out_channels)
#         W(Tensor): Spectrally normalized weight
#         u (Tensor): the right largest singular value of W.
#     .. _cross-correlation:
#         https://en.wikipedia.org/wiki/Cross-correlation
#     .. _link:
#         https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
#     """

#     def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):
#         kernel_size = _pair(kernel_size)
#         stride = _pair(stride)
#         padding = _pair(padding)
#         dilation = _pair(dilation)
#         super(SNConv2d, self).__init__(
#             in_channels, out_channels, kernel_size, stride, padding, dilation,
#             False, _pair(0), groups, bias, padding_mode='zeros')
#         self.register_buffer('u', torch.Tensor(1, out_channels).normal_())

#     @property
#     def W_(self):
#         w_mat = self.weight.view(self.weight.size(0), -1)
#         sigma, _u = max_singular_value(w_mat, self.u)
#         self.u.copy_(_u)
#         return self.weight / sigma

#     def forward(self, input):
#         return F.conv2d(input, self.W_, self.bias, self.stride,
#                         self.padding, self.dilation, self.groups)


# class SNLinear(Linear):
#     r"""Applies a linear transformation to the incoming data: :math:`y = Ax + b`
#        Args:
#            in_features: size of each input sample
#            out_features: size of each output sample
#            bias: If set to False, the layer will not learn an additive bias.
#                Default: ``True``
#        Shape:
#            - Input: :math:`(N, *, in\_features)` where :math:`*` means any number of
#              additional dimensions
#            - Output: :math:`(N, *, out\_features)` where all but the last dimension
#              are the same shape as the input.
#        Attributes:
#            weight: the learnable weights of the module of shape
#                `(out_features x in_features)`
#            bias:   the learnable bias of the module of shape `(out_features)`
#            W(Tensor): Spectrally normalized weight
#            u (Tensor): the right largest singular value of W.
#        """

#     def __init__(self, in_features, out_features, bias=True):
#         super(SNLinear, self).__init__(in_features, out_features, bias)
#         self.register_buffer('u', torch.Tensor(1, out_features).normal_())

#     @property
#     def W_(self):
#         w_mat = self.weight.view(self.weight.size(0), -1)
#         sigma, _u = max_singular_value(w_mat, self.u)
#         self.u.copy_(_u)
#         return self.weight / sigma

#     def forward(self, input):
#         return F.linear(input, self.W_, self.bias)


class Generator(nn.Module):
    """
    Input shape: (N, in_dim)
    Output shape: (N, 3, 64, 64)
    """

    def __init__(self, in_dim, dim=64):
        super(Generator, self).__init__()

        def dconv_bn_relu(in_dim, out_dim):
            return nn.Sequential(
                # nn.ConvTranspose2d(in_dim, out_dim, 5, 2,
                #                    padding=2, output_padding=1, bias=False),
                nn.Upsample(scale_factor=2),
                nn.ReflectionPad2d(2),
                nn.Conv2d(in_dim, out_dim, 5),
                nn.BatchNorm2d(out_dim),
                nn.ReLU()
            )
        self.l1 = nn.Sequential(
            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),
            nn.BatchNorm1d(dim * 8 * 4 * 4),
            nn.ReLU()
        )
        self.l2_5 = nn.Sequential(
            dconv_bn_relu(dim * 8, dim * 4),
            dconv_bn_relu(dim * 4, dim * 2),
            dconv_bn_relu(dim * 2, dim),
            # nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),
            nn.Upsample(scale_factor=2),
            nn.ReflectionPad2d(2),
            nn.Conv2d(dim, 3, 5),
            nn.Tanh()
        )
        self.apply(weights_init)

    def forward(self, x):
        y = self.l1(x)
        y = y.view(y.size(0), -1, 4, 4)
        y = self.l2_5(y)
        return y


class Discriminator(nn.Module):
    """
    Input shape: (N, 3, 64, 64)
    Output shape: (N, )
    """

    def __init__(self, in_dim, dim=64):
        super(Discriminator, self).__init__()

        def conv_bn_lrelu(in_dim, out_dim):
            return nn.Sequential(
                # nn.Conv2d(in_dim, out_dim, 5, 2, 2),
                nn.utils.spectral_norm(nn.Conv2d(in_dim, out_dim, 5, 2, 2)),
                # nn.BatchNorm2d(out_dim),
                nn.LeakyReLU(0.1),
            )

        """ Medium: Remove the last sigmoid layer for WGAN. """
        self.ls = nn.Sequential(
            # nn.Conv2d(in_dim, dim, 5, 2, 2),
            nn.utils.spectral_norm(nn.Conv2d(in_dim, dim, 5, 2, 2)),
            nn.LeakyReLU(0.1),
            conv_bn_lrelu(dim, dim * 2),
            conv_bn_lrelu(dim * 2, dim * 4),
            conv_bn_lrelu(dim * 4, dim * 8),
            # nn.Conv2d(dim * 8, 1, 4),
            nn.utils.spectral_norm(nn.Conv2d(dim * 8, 1, 4)),
            nn.Sigmoid(),
        )
        self.apply(weights_init)

    def forward(self, x):
        y = self.ls(x)
        y = y.view(-1)
        return y

# class _netG(nn.Module):
#     def __init__(self, nz, nc, ngf):
#         super(_netG, self).__init__()
#         self.main = nn.Sequential(
#             # input is Z, going into a convolution
#             nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=True),
#             nn.BatchNorm2d(ngf * 8),
#             nn.ReLU(True),
#             # state size. (ngf*8) x 4 x 4
#             nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=True),
#             nn.BatchNorm2d(ngf * 4),
#             nn.ReLU(True),
#             # state size. (ngf*4) x 8 x 8
#             nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=True),
#             nn.BatchNorm2d(ngf * 2),
#             nn.ReLU(True),
#             # state size. (ngf*2) x 16 x 16
#             nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=True),
#             nn.BatchNorm2d(ngf),
#             nn.ReLU(True),
#             # state size. (ngf) x 32 x 32
#             nn.ConvTranspose2d(ngf, nc, 3, 1, 1, bias=True),
#             nn.Tanh()
#             # state size. (nc) x 32 x 32
#         )

#     def forward(self, input):
#         output = self.main(input)
#         return output


# class _netD(nn.Module):
#     def __init__(self, nc, ndf):
#         super(_netD, self).__init__()

#         self.main = nn.Sequential(
#             # input is (nc) x 32 x 32
#             # SNConv2d()
#             SNConv2d(nc, ndf, 3, 1, 1, bias=True),
#             nn.LeakyReLU(0.1, inplace=True),
#             SNConv2d(ndf, ndf, 4, 2, 1, bias=True),
#             nn.LeakyReLU(0.1, inplace=True),
#             # state size. (ndf) x 1 x 32
#             SNConv2d(ndf, ndf * 2, 3, 1, 1, bias=True),
#             nn.LeakyReLU(0.1, inplace=True),
#             SNConv2d(ndf*2, ndf * 2, 4, 2, 1, bias=True),
#             #nn.BatchNorm2d(ndf * 2),
#             nn.LeakyReLU(0.1, inplace=True),
#             # state size. (ndf*2) x 16 x 16
#             SNConv2d(ndf * 2, ndf * 4, 3, 1, 1, bias=True),
#             nn.LeakyReLU(0.1, inplace=True),
#             SNConv2d(ndf * 4, ndf * 4, 4, 2, 1, bias=True),
#             nn.LeakyReLU(0.1, inplace=True),
#             # state size. (ndf*8) x 4 x 4
#             SNConv2d(ndf * 4, ndf * 8, 3, 1, 1, bias=True),
#             nn.LeakyReLU(0.1, inplace=True),
#             SNConv2d(ndf * 8, 1, 4, 1, 0, bias=False),
#             nn.Sigmoid()
#         )
#         # self.snlinear = nn.Sequential(SNLinear(ndf * 4 * 4 * 4, 1),
#         #                              nn.Sigmoid())

#     def forward(self, input):
#         output = self.main(input)
#         #output = output.view(output.size(0), -1)
#         #output = self.snlinear(output)
#         return output.view(-1, 1).squeeze(1)


"""## Create trainer
In this section, we will create a trainer which contains following functions:
1. prepare_environment: prepare the overall environment, construct the models, create directory for the log and ckpt
2. train: train for generator and discriminator, you can try to modify the code here to construct WGAN or WGAN-GP
3. inference: after training, you can pass the generator ckpt path into it and the function will save the result for you

## Training

### Initialization
- hyperparameters
- model
- optimizer
- dataloader
"""

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
device = "cuda" if torch.cuda.is_available() else "cpu"
# Training hyperparameters
batch_size = 128
z_dim = 150
z_sample = Variable(torch.randn(150, z_dim)).to(device)
lr = 0.0004
# 0.0004

""" Medium: WGAN, 50 epoch, n_critic=5, clip_value=0.01 """
n_epoch = 100  # 50
n_critic = 1  # 5
clip_value = 0.01

log_dir = os.path.join(workspace_dir, 'logs_SNGAN2')
# log_dir = os.path.join('/content/gdrive/My Drive/HW6', 'logs')
ckpt_dir = os.path.join(workspace_dir, 'checkpoints_SNGAN2')
# ckpt_dir = os.path.join('/content/gdrive/My Drive/HW6', 'checkpoints')
os.makedirs(log_dir, exist_ok=True)
os.makedirs(ckpt_dir, exist_ok=True)

# Model
G = Generator(in_dim=z_dim).to(device)
# G = _netG(z_dim, 3, 64).to(device)
D = Discriminator(3).to(device)
# D = _netD(3, 64).to(device)
G.train()
D.train()

# Loss
criterion = nn.BCELoss()

""" Medium: Use RMSprop for WGAN. """
# Optimizer
opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0, 0.999))
opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0, 0.999))
# opt_D = torch.optim.RMSprop(D.parameters(), lr=lr)
# opt_G = torch.optim.RMSprop(G.parameters(), lr=lr)

# DataLoader
dataloader = DataLoader(dataset, batch_size=batch_size,
                        shuffle=True, num_workers=4)

"""### Training loop
We store some pictures regularly to monitor the current performance of the Generator, and regularly record checkpoints.
"""
G.load_state_dict(torch.load(os.path.join(ckpt_dir, 'G_SNGAN2270.pth')))
D.load_state_dict(torch.load(os.path.join(ckpt_dir, 'D_SNGAN2270.pth')))
opt_G.load_state_dict(torch.load(
    os.path.join(ckpt_dir, 'opt_G_SNGAN2270.pth')))
opt_D.load_state_dict(torch.load(
    os.path.join(ckpt_dir, 'opt_D_SNGAN2270.pth')))

steps = 0
for e, epoch in enumerate(range(n_epoch)):
    # progress_bar = qqdm(dataloader)
    # for i, data in enumerate(progress_bar):
    for i, data in enumerate(dataloader):
        imgs = data
        imgs = imgs.to(device)

        bs = imgs.size(0)

        # ============================================
        #  Train D
        # ============================================
        z = Variable(torch.randn(bs, z_dim)).to(device)
        # image add noise
        r_imgs = Variable(imgs).to(device) + 0.02 * \
            torch.randn(bs, 3, 64, 64).to(device)
        f_imgs = G(z)
        # f_imgs = G(z, 3, 64)

        """ Medium: Use WGAN Loss. """
        # Label
        # r_label = torch.ones((bs)).to(device)
        # f_label = torch.zeros((bs)).to(device)
        r_label = Variable(torch.cuda.FloatTensor(
            np.random.uniform(0.8, 1, bs)))
        f_label = Variable(torch.cuda.FloatTensor(
            np.random.uniform(0.0, 0.2, bs)))

        # Model forwarding
        r_logit = D(r_imgs.detach())
        f_logit = D(f_imgs.detach())
        # r_logit = D(r_imgs.detach(), 64)
        # f_logit = D(f_imgs.detach(), 64)

        # Compute the loss for the discriminator.
        r_loss = criterion(r_logit, r_label)
        f_loss = criterion(f_logit, f_label)
        loss_D = (r_loss + f_loss) / 2

        # WGAN Loss
        # loss_D = -torch.mean(D(r_imgs)) + torch.mean(D(f_imgs))

        # Model backwarding
        D.zero_grad()
        loss_D.backward()

        # Update the discriminator.
        opt_D.step()

        # """ Medium: Clip weights of discriminator. """
        # for p in D.parameters():
        #     p.data.clamp_(-clip_value, clip_value)

        # ============================================
        #  Train G
        # ============================================
        if steps % n_critic == 0:
            # Generate some fake images.
            z = Variable(torch.randn(bs, z_dim)).to(device)
            r_imgs = Variable(imgs).to(device) + 0.02 * \
                torch.randn(bs, 3, 64, 64).to(device)
            f_imgs = G(z)

            mean_r = torch.mean(r_imgs, 1)
            mean_f = torch.mean(f_imgs, 1)
            std_r = torch.std(r_imgs, 1)
            std_f = torch.std(f_imgs, 1)
            loss_mse = nn.MSELoss(mean_f, mean_r)/3*2
            loss_mse_std = nn.MSELoss(std_f, std_r)/3
            # Model forwarding
            f_logit = D(f_imgs)

            """ Medium: Use WGAN Loss"""
            # Compute the loss for the generator.
            loss_G = criterion(f_logit, r_label)/3 + loss_mse + loss_mse_std
            # WGAN Loss
            # loss_G = -torch.mean(D(f_imgs))

            # Model backwarding
            G.zero_grad()
            loss_G.backward()

            # Update the generator.
            opt_G.step()

        steps += 1

        # Set the info of the progress bar
        #   Note that the value of the GAN loss is not directly related to
        #   the quality of the generated images.
        # progress_bar.set_infos({
        #     'Loss_D': round(loss_D.item(), 4),
        #     'Loss_G': round(loss_G.item(), 4),
        #     'Epoch': e+1,
        #     'Step': steps,
        # })
        print('Loss_D: ', round(loss_D.item(), 4))
        print('Loss_G: ', round(loss_G.item(), 4))
        print('Epoch: ', e+1)
        print('Step: ', steps)

    G.eval()
    f_imgs_sample = (G(z_sample).data + 1) / 2.0
    filename = os.path.join(log_dir, f'Epoch_{(epoch+270)+1:03d}.jpg')
    torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)
    print(f' | Save some samples to {filename}.')

    # # Show generated images in the jupyter notebook.
    # grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)
    # plt.figure(figsize=(10, 10))
    # plt.imshow(grid_img.permute(1, 2, 0))
    # plt.show()
    G.train()

    if (e+1) % 10 == 0 or e == 0:
        # Save the checkpoints.
        torch.save(G.state_dict(), os.path.join(
            ckpt_dir, f'G_SNGAN2{int(e+271)}.pth'))
        torch.save(D.state_dict(), os.path.join(
            ckpt_dir, f'D_SNGAN2{int(e+271)}.pth'))
        torch.save(opt_D.state_dict(), os.path.join(
            ckpt_dir, f'opt_D_SNGAN2{int(e+271)}.pth'))
        torch.save(opt_G.state_dict(), os.path.join(
            ckpt_dir, f'opt_G_SNGAN2{int(e+271)}.pth'))
    torch.save(G.state_dict(), os.path.join(ckpt_dir, 'G_SNGAN2.pth'))
    torch.save(D.state_dict(), os.path.join(ckpt_dir, 'D_SNGAN2.pth'))
"""## Inference
Use the trained model to generate anime faces!

### Load model
"""

G = Generator(z_dim)
G.load_state_dict(torch.load(os.path.join(ckpt_dir, 'G_SNGAN2.pth')))
G.eval()
G.to(device)
# D = Discriminator(3)
# D.load_state_dict(torch.load(os.path.join(ckpt_dir, 'D_SNGAN2.pth')))
# D.eval()
# D.to(device)

"""### Generate and show some images.

"""

# Generate 1000 images and make a grid to save them.
n_output = 1000
z_sample = Variable(torch.randn(n_output, z_dim)).to(device)
imgs_sample = (G(z_sample).data + 1) / 2.0
log_dir = os.path.join(workspace_dir, 'logs_SNGAN2')
# log_dir = os.path.join('/content/gdrive/My Drive/HW6', 'logs')
filename = os.path.join(log_dir, 'result_SNGAN2.jpg')
torchvision.utils.save_image(imgs_sample, filename, nrow=10)

# Show 32 of the images.
grid_img = torchvision.utils.make_grid(imgs_sample[:32].cpu(), nrow=8)
plt.figure(figsize=(10, 10))
plt.imshow(grid_img.permute(1, 2, 0))
plt.show()

"""### Compress the generated images using **tar**.

"""

# Commented out IPython magic to ensure Python compatibility.
# Save the generated images.
os.makedirs('output_SNGAN2', exist_ok=True)
# os.makedirs('/content/gdrive/My Drive/HW6',exist_ok=True)
imgs_sample = (imgs_sample.permute(0, 2, 3, 1) *
               255).cpu().numpy().astype('uint8')  # 多加這行
for i in range(n_output):
    im = Image.fromarray(imgs_sample[i])
    im.save(f"output_SNGAN2/{i+1}.jpg", quality=86, subsampling=0)  # 200_2.tgz
    # torchvision.utils.save_image(imgs_sample[i], f'output_SNGAN2/{i+1}.jpg')

# import tarfile
# # Compress the images.
# # %cd output
# tar -zcf ../images.tgz *.jpg
# # %cd ..

# n_output, n_gen = 1000, 2000
# z_sample = Variable(torch.randn(n_gen, z_dim)).to(device)
# imgs_sample = []
# imgs_vec = []
# imgs_score = []
# with torch.no_grad():
#     for i in range(0, n_gen, 50):
#         img = G(z_sample[i:i+50])
#         imgs_sample.append(img)
#         for j in D.ls[:5]:
#             img = j(img)  # apply until before last conv
#         imgs_vec.append(img)
#         # by_score
#         for j in D.ls[5:]:
#             img = j(img)
#         imgs_score.append(img.view(-1))
# imgs_sample = (torch.cat(imgs_sample) + 1) / 2.0
# imgs_vec = torch.cat(imgs_vec)
# # by_score
# imgs_score = torch.cat(imgs_score)
# log_dir = os.path.join(workspace_dir, 'log_SNGAN2')

# # by_feature & by_image
# # imgs_flatten = imgs_sample.view(n_gen, -1).clone()  # by_image
# # imgs_flatten = imgs_vec.view(n_gen, -1)            # by_feature
# # imgs_flatten /= torch.std(imgs_flatten, 1, keepdim=True) + 1e-6
# # dist = torch.cdist(imgs_flatten, imgs_flatten)
# # dist_n = torch.sum(torch.sort(dist, -1)[0][:, :50], -1)
# # dist = dist.cpu().numpy()
# # dist_n = dist_n.cpu().numpy()
# # idx = set(range(n_gen))
# # lst = sorted([(dist[i, j], i, j) for i in range(n_gen) for j in range(i)])
# # for _, i, j in lst:
# #     if i not in idx:
# #         if j not in idx:
# #             continue
# #         idx.remove(j)
# #     elif j not in idx or dist_n[i] < dist_n[j]:
# #         idx.remove(i)
# #     else:
# #         idx.remove(j)
# #     grid_img = torchvision.utils.make_grid(
# #         imgs_sample[[i, j]].cpu(), nrow=2).permute(1, 2, 0)
# #     #plt.imsave("sim/test{:03d}.png".format(n_gen - len(idx)), grid_img.numpy())
# #     if len(idx) == n_output:
# #         break

# # by_score
# idx = torch.sort(imgs_score)[1][:n_output].cpu().tolist()
# imgs_sample = imgs_sample[list(idx)]

# imgs_sample = (imgs_sample.permute(0, 2, 3, 1) *
#                255).cpu().numpy().astype('uint8')
# for i in range(n_output):
#     im = Image.fromarray(imgs_sample[i])
#     im.save(f"output_SNGAN2/{i+1}.jpg", quality=86, subsampling=0)
