# -*- coding: utf-8 -*-
"""DLCVHW1_p2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IADKvUztyDltnc10s_UYM75r-eubMG7g

# Part 2
"""

# !gdown --id 1LMIaOY8NSKWmGtbvTsjXcHVaYKnNN_9u --output "data.zip"
# ! unzip data.zip

# !pip install -q segmentation-models-pytorch
# !pip install -q torchsummary

import os
import glob
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F

import torch.utils.data as data
import torchvision.transforms as transforms
from torchvision import datasets
import torchvision.models as models
from torchvision.datasets import DatasetFolder
from torchvision.models import vgg16
from torch import optim
from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import random
import csv
import pandas as pd
import imageio
from tqdm.auto import tqdm

torch.cuda.empty_cache()

def read_masks(filepath):
    '''
    Read masks from directory and tranform to categorical
    '''
    file_list = [file for file in os.listdir(filepath) if file.endswith('.png')]
    file_list.sort()
    n_masks = len(file_list)
    masks = np.empty((n_masks, 512, 512))

    for i, file in enumerate(file_list):
        mask = imageio.imread(os.path.join(filepath, file))
        mask = (mask >= 128).astype(int)
        mask = 4 * mask[:, :, 0] + 2 * mask[:, :, 1] + mask[:, :, 2]
        masks[i, mask == 3] = 0  # (Cyan: 011) Urban land 
        masks[i, mask == 6] = 1  # (Yellow: 110) Agriculture land 
        masks[i, mask == 5] = 2  # (Purple: 101) Rangeland 
        masks[i, mask == 2] = 3  # (Green: 010) Forest land 
        masks[i, mask == 1] = 4  # (Blue: 001) Water 
        masks[i, mask == 7] = 5  # (White: 111) Barren land 
        masks[i, mask == 0] = 6  # (Black: 000) Unknown 

    return masks

def read_sats(filepath):
    '''
    Read masks from directory and tranform to categorical
    '''
    file_list = [file for file in os.listdir(filepath) if file.endswith('.jpg')]
    file_list.sort()
    n_masks = len(file_list)
    sats = []

    for i, file in enumerate(file_list):
        sat = Image.open(os.path.join(filepath, file))
        sats.append(test_tfm(sat))
        sat.close()

    return sats

def mean_iou_score(pred, labels):
    '''
    Compute mean IoU score over 6 classes
    '''
    mean_iou = 0
    for i in range(6):
        tp_fp = np.sum(pred == i)
        tp_fn = np.sum(labels == i)
        tp = np.sum((pred == i) * (labels == i))
        iou = tp / (tp_fp + tp_fn - tp)
        mean_iou += iou / 6
        print('class #%d : %1.5f'%(i, iou))
    print('\nmean_iou: %f\n' % mean_iou)

    return mean_iou

# class p2Data(Dataset):
#     def __init__(self, root, images, labels, transform=None):
#         super(p2Data, self).__init__()
#         self.transform = transform
#         self.images = images
#         self.labels = labels
#         self.filenames = []
#         filenames = glob.glob(os.path.join(root,'*'))
#         for fn in filenames:
#             self.filenames.append(fn)
#         self.len = len(self.filenames)
#     def __getitem__(self, index):
#         image_fn = self.images[index]
#         image = Image.open(image_fn)
#         if self.transform is not None:
#             image = self.transform(image)
#         label = self.labels[index]
#         return image, label
#     def __len__(self):
#         return self.len

class p2Data(Dataset):
    def __init__(self, images, labels):
        super(p2Data, self).__init__()
        # self.transform = transform
        self.images = images
        self.labels = labels
        self.len = len(self.labels)
    def __getitem__(self, index):
        image = self.images[index]
        # image = Image.open(image)
        label = self.labels[index]
        # if self.transform is not None:
        #     image = self.transform(image)
        return image, label
    def __len__(self):
        return self.len

train_tfm = transforms.Compose([
	transforms.Resize((256, 256)),
  transforms.ColorJitter(0.3, 0.2, 0.15),
  transforms.RandomHorizontalFlip(p=0.6),
  transforms.RandomAffine(degrees=(-20, 20), translate=(0.1, 0.1), scale=(0.8, 1.2)),
    transforms.ToTensor(),
  # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

test_tfm = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Set path, batch size
batch_size = 24
train_path = "./hw1_data/p2_data/train/"
validation_path = "./hw1_data/p2_data/validation/"
train_image_length = 2000
val_image_length = 257
train_images = []
train_label = []
val_images = []
val_label = []

# Construct datasets
train_images = read_sats(train_path)
train_label = read_masks(train_path)
val_images = read_sats(validation_path)
val_label = read_masks(validation_path)
train_set = p2Data(images=train_images, labels=train_label)
validation_set = p2Data(images=val_images, labels=val_label)

print('# images in trainset:', len(train_set)) 
print('# images in validationset:', len(validation_set))

# Construct data loaders.
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
valid_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

# Baseline Model (Model A)
class fcn32(nn.Module):
    """
    ref:https://github.com/sairin1202/fcn32-pytorch/blob/master/pytorch-fcn32.py
    """
    def __init__(self, backbone_model=models.vgg16(pretrained=True)):
        super(fcn32,self).__init__()
        self.features = backbone_model.features
        self.conv6 = nn.Conv2d(512, 512, 3, 1, 1)
        self.conv7 = nn.Conv2d(512, 512, 3, 1, 1)
        self.upsample = nn.ConvTranspose2d(512, 7, 128, 64, 0, bias=False)
        self.dropout = nn.Dropout2d()
        self.relu = nn.ReLU(inplace=True)

    def forward(self,x):
        x = self.features(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.conv6(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.conv7(x)
        x = self.upsample(x)
        x = x[:, :, 32:x.shape[2]-32, 32:x.shape[3]-32]
        return x

# class segnet(nn.Module):
#     """
#     ref:https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html
#     https://foundationsofdl.com/2021/03/03/segmentation-model-implementation/
#     """
#     def __init__(self, backbone_model=models.vgg16(pretrained=True)):
#         super(segnet,self).__init__()
#         self.features = backbone_model.features
#         self.conv6 = nn.Conv2d(512, 512, 3, 1, 1)
#         self.conv7 = nn.Conv2d(512, 512, 3, 1, 1)
#         self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=None)
#         self.dropout = nn.Dropout2d()
#         self.relu = nn.ReLU(inplace=True)
        
#         self.conv_decoder1 = nn.Conv2d(512, 256, 3, 1, 1)
#         self.conv_decoder2 = nn.Conv2d(256, 128, 3, 1, 1)
#         self.conv_decoder3 = nn.Conv2d(128, 128, 3, 1, 1)
#         self.conv_decoder4 = nn.Conv2d(128, 64, 3, 1, 1)
#         self.conv_decoder5 = nn.Conv2d(64, 32, 3, 1, 1)
#         self.conv_decoder6 = nn.Conv2d(32, 7, 3, 1, 1)

#     def forward(self,x):
#         x = self.features(x)
#         x = self.relu(x)
#         x = self.dropout(x)
#         x = self.conv6(x)
#         x = self.relu(x)
#         x = self.dropout(x)
#         x = self.conv7(x)
#         x = self.relu(x)
#         x = self.upsample2(x)
#         x = self.conv_decoder1(x)
#         x = self.relu(x)
#         x = self.upsample2(x)
#         x = self.conv_decoder2(x)
#         x = self.relu(x)
#         x = self.upsample2(x)
#         x = self.conv_decoder3(x)
#         x = self.relu(x)
#         x = self.upsample2(x)
#         x = self.conv_decoder4(x)
#         x = self.relu(x)
#         x = self.upsample2(x)
#         x = self.conv_decoder5(x)
#         x = self.relu(x)
#         x = self.upsample2(x)
#         x = self.conv_decoder6(x)
#         x = self.relu(x)
#         return x

# Improved Model (Model B)
class fcn4(nn.Module):
    """
    ref:https://foundationsofdl.com/2021/03/03/segmentation-model-implementation/
    """
    def __init__(self, backbone_model=models.vgg16(pretrained=True)):
        super(fcn4, self).__init__()
        self.features = backbone_model.features
        self.conv1to2 = nn.Sequential(*list(self.features.children())[:11])
        self.poo12 = self.features[10]
        self.conv3 = nn.Sequential(*list(self.features.children())[11:17]) 
        self.pool3 = self.features[16]
        self.conv4 = nn.Sequential(*list(self.features.children())[17:24])
        self.pool4 = self.features[23]
        self.conv5 = nn.Sequential(*list(self.features.children())[24:])
        self.pool5 = self.features[30]
        self.conv6 = nn.Conv2d(512, 512, 3, 1, 1)
        self.conv7 = nn.Conv2d(512, 512, 3, 1, 1)
        self.upsample2 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, bias=False)
        self.upsample2_fuse = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, bias=False)
        self.upsample4 = nn.ConvTranspose2d(512, 256, kernel_size=8, stride=4, bias=False)
        # self.upsample8 = nn.ConvTranspose2d(512, 256, kernel_size=16, stride=8, bias=False)
        self.upsample8 = nn.ConvTranspose2d(256, 7, kernel_size=16, stride=8, bias=False)
        # self.upsample16 = nn.ConvTranspose2d(256, 7, 32, 16, 0, bias=False)
        # self.upsample32 = nn.ConvTranspose2d(512, 7, 64, 32, 0, bias=False)
        self.dropout = nn.Dropout2d()
        self.relu = nn.ReLU(inplace=True)

    def forward(self,x):
        x = self.conv1to2(x)
        pooling2 = x
        x = self.conv3(x)
        pooling3 = x
        x = self.conv4(x)
        pooling4 = x
        pooling4 = self.upsample2(pooling4)
        x = self.conv5(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.conv6(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.conv7(x)
        conv_7 = x
        # print('conv7',conv_7.shape, conv_7.dtype)
        conv_7 = self.upsample4(conv_7)
        # print('conv7',conv_7.shape, conv_7.dtype)
        fusion_1 = pooling3 + pooling4[:, :, 1:pooling4.shape[2]-1, 1:pooling4.shape[3]-1] + conv_7[:, :, 2:conv_7.shape[2]-2, 2:conv_7.shape[3]-2]
        # print('pool3', pooling3.shape, pooling3.dtype)
        fusion_1 = self.upsample2_fuse(fusion_1)
        fusion_2 = pooling2 + fusion_1[:, :, 1:fusion_1.shape[2]-1, 1:fusion_1.shape[3]-1]
        # print("fusion_2",fusion_2.shape, fusion_2.dtype)
        fusion = self.upsample8(fusion_2)
        x_out = fusion[:, :, 4:fusion.shape[2]-4, 4:fusion.shape[3]-4]
        # print(x_out.shape, x_out.dtype)
        return x_out


# from PIL import ImageFile
# ImageFile.LOAD_TRUNCATED_IMAGES = True

device = "cuda" if torch.cuda.is_available() else "cpu"
# model = fcn32.to(device)
# model = segnet().to(device)
model = fcn4().to(device)
model_path = './model_p2_fcn4_deep.ckpt'
model_path_early = './model_p2_fcn4_deep_early.ckpt'
model_path_middle = './model_p2_fcn4_deep_middle.ckpt'
model_path_late = './model_p2_fcn4_deep_late.ckpt'

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5, betas=(0.9, 0.999))
miou_list = []
n_epochs = 1000
best_miou = 0
stale = 0
early_stop = 1000
# checkpoint = torch.load('drive/MyDrive/model.pt')
# model.load_state_dict(checkpoint['model_state_dict'])
# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

for epoch in range(n_epochs):
    model.train()
    predicted_mask = []
    train_label = []
    train_loss = []
    for batch in tqdm(train_loader):
        imgs, labels = batch
        logits = model(imgs.to(device))

        labels = torch.tensor(labels, dtype=torch.long)
        loss = criterion(logits, labels.to(device))
        # loss = checkpoint['loss']
        optimizer.zero_grad()
        loss.backward()
        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)
        optimizer.step()
        # acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()
        predicted_mask.extend(logits.argmax(dim=1).cpu().detach().numpy())
        train_label.extend(labels.cpu().detach().numpy())
        train_loss.append(loss.item())

    train_loss = sum(train_loss) / len(train_loss)
    predicted_mask = np.array(predicted_mask, dtype=np.float64)
    train_label = np.array(train_label, dtype=np.float64)
    train_miou = mean_iou_score(predicted_mask, train_label)
    print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, mIoU = {train_miou:.5f}")

    model.eval()
    predicted_mask_val = []
    # valid_loss = []
    for batch in tqdm(valid_loader):
        torch.cuda.empty_cache()
        imgs, labels = batch
        with torch.no_grad():
          logits = model(imgs.to(device))
        # loss = criterion(logits, labels.to(device))
        predicted_mask_val.extend(logits.argmax(dim=1).cpu().detach().numpy())
        # val_label.extend(labels.cpu().detach().numpy())
        # valid_loss.append(loss.item())

    # valid_loss = sum(valid_loss) / len(valid_loss)
    predicted_mask_val = np.array(predicted_mask_val, dtype=np.float64)
    valid_miou = mean_iou_score(predicted_mask_val, val_label)
    # print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_miou:.5f}")
    if epoch == 2:
        state = {'net': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}
        torch.save(state, model_path_early)
        print('Early step: saving model with mIoU {:.3f}'.format(valid_miou))
    if epoch == 14:
        state = {'net': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}
        torch.save(state, model_path_middle)
        print('Middle step: saving model with mIoU {:.3f}'.format(valid_miou))
    if epoch == 35:
        state = {'net': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}
        torch.save(state, model_path_late)
        print('Late step: saving model with mIoU {:.3f}'.format(valid_miou))
    # Save best model
    if valid_miou > best_miou:
        best_miou = valid_miou
        state = {'net': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}
        torch.save(state, model_path)
        print('saving model with mIoU {:.3f}'.format(best_miou))
        stale = 0
    else:
        stale += 1
        if stale > early_stop:
            print(f"No improvment for {early_stop} epochs, stop training!")
            break
    miou_list.append(valid_miou)